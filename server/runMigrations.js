const fs = require('fs');
const path = require('path');
const { Pool } = require('pg');

// Database connection
const pool = new Pool({
  connectionString: process.env.DATABASE_URL || 'postgresql://postgres:postgres@localhost:5432/biolab',
  ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false
});

const migrationsDir = path.join(__dirname, 'migrations');

async function runMigrations() {
  console.log('üöÄ Starting database migrations...\n');

  try {
    // Get all migration files
    const files = fs.readdirSync(migrationsDir)
      .filter(f => f.endsWith('.sql'))
      .sort();

    console.log(`Found ${files.length} migration files:\n`);
    files.forEach((f, i) => console.log(`  ${i + 1}. ${f}`));
    console.log('');

    for (const file of files) {
      const filePath = path.join(migrationsDir, file);
      const sql = fs.readFileSync(filePath, 'utf8');

      console.log(`üì¶ Running migration: ${file}`);

      try {
        await pool.query(sql);
        console.log(`‚úÖ ${file} completed successfully\n`);
      } catch (error) {
        if (error.message.includes('already exists')) {
          console.log(`‚ö†Ô∏è  ${file} - tables already exist (skipping)\n`);
        } else {
          console.error(`‚ùå ${file} failed:`, error.message);
          throw error;
        }
      }
    }

    console.log('‚úÖ All migrations completed successfully!\n');

    // Verify tables
    console.log('üîç Verifying database tables...');
    const result = await pool.query(`
      SELECT table_name
      FROM information_schema.tables
      WHERE table_schema = 'public'
      ORDER BY table_name
    `);

    console.log(`\nüìä Found ${result.rows.length} tables:`);
    result.rows.forEach((row, i) => {
      console.log(`  ${i + 1}. ${row.table_name}`);
    });

    console.log('\n‚úÖ Database is ready!');

  } catch (error) {
    console.error('\n‚ùå Migration failed:', error);
    process.exit(1);
  } finally {
    await pool.end();
  }
}

// Run migrations
runMigrations();
